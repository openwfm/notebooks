{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3f116-148d-4251-a8a5-e908710bc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from moisture_rnn import staircase, staircase_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8155370-6650-4d18-b3e4-c39260d217b4",
   "metadata": {},
   "source": [
    "# Input Data Structure for RNNs\n",
    "\n",
    "## Background\n",
    "\n",
    "RNNs are a type of timeseries model that relates the outcome at time $t$ to the outcome at previous times. Like other machine learning models, training is typically done by calculating the gradient of the output with respect to the weights, or parameters, of the model. With recursive or other type of autoregressive models, the gradient calculation at time $t$ ends up depending on the gradient at $t-1, t-2, ...,$ and to $t=0$. This ends up being computationally expensive, but more importantly can lead to \"vanishing\" or \"exploding\" gradient problems, where many gradients are multiplied together and either blow up or shrink. See LINK_TO_RECURSIVE_GRADIENT_LATEX for more info...\n",
    "\n",
    "RRNs and other timeseries neural network architectures* get around this issue by approximating the gradient in more stable ways. In addition to many model architecture and hyperparameter options, timeseries neural networks use two main ways of restructuring the input data.\n",
    "\n",
    "* **Sequence Length:** The input data is divided into smaller collections of ordered events, known as sequences. When calculating the gradient with respect to the model weights, the gradient only looks back this number of timesteps. Also known as `timesteps`, `sequence_length`, or just \"sample\" in `tensorflow` functions and related literature. For a sequence length of 3, the data would be broken up as: `[1,2,3], [2,3,4], ..., [T-2, T-1, T]`, for a total number of sequences `T-timesteps+1`\n",
    "\n",
    "* **Batch size:** Sequences are grouped together into batches. The batch size determines how many sequences the network processes in a single step of calculating loss and updating weights. Used as `batch_size` in `tensorflow`.\n",
    "\n",
    "The total number of batches is therefore determined from the total number of observations $T$ and the batch size. The function `timeseries_dataset_from_array` from `keras.preprocessing` can be used for this purpose, and we will compare that to a custom function `staircase` that demonstrates the method in detail.\n",
    "\n",
    "**Note:* these same data principles apply to more complex versions of timeseries neural network layers, such as LSTM and GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa2a9d-c74b-445d-a875-e7591c7a6666",
   "metadata": {},
   "source": [
    "## Stateless vs Stateful Networks\n",
    "\n",
    "RNNs have a hidden state that represents the recurrent layer output at a previous time. There is a weight and bias at each RNN cell that determines the relative contribution of the previous output to the current output. When updating weights in RNNs, there are two main types of training scheme:\n",
    "\n",
    "**Stateless:** the hidden state is reset to the initial state (often zero) at the start of each new sequence in a batch. So, the network treats each sequence independently, and no information is carried over in time between sequences. These models are simpler, but work better when time dependence is relatively short.\n",
    "* **Input Data Shape:** (`n_sequence`, `timesteps`, `features`), where `n_sequence` is total number of sequences (a function of total observed times `T` and the user choice of timesteps). The input data does NOT need to be structured with batch size in stateless RNNs.\n",
    "* **Tensorflow RNN Args:** for a stateless RNN, use the `input_shape` parameter, with `input_shape`=(`timesteps`, `features`). Then, `batch_size` can be declared in the fitting stage with `model.fit(X_train, batch_size = __)`. \n",
    "\n",
    "**Stateful:** the hidden states are carried over from one sequence to the next within a batch. Longer time dependencies can be learned in this way.\n",
    "* **Input Data Shape:** (`batch_size`, `timesteps`, `features`). In order for the hidden state to be passed between sequences, the input data must be formatted using the `batch_size` hyperparameter.\n",
    "* **Tensorflow RNN Args:** for a stateful RNN, use the `batch_input_shape` parameter, with `batch_input_shape`=(`batch_size`, `timesteps`, `features`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e9011c-ee92-4bc2-bf6b-461b5bf9c662",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Data Description\n",
    "\n",
    "Consider $T=100$ observations of a variable in time $y_t$, so $t=1, ..., 100$. A feature matrix with $3$ features has dimensions $100\\times 3$, and must be restructured for use in RNNs. In the code below each feature is a sequential list 1, 2, ..., 100 (since python numbering starts at 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03048248-0c81-491d-b0ce-a099bd984dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "T=100 # total number of times obseved\n",
    "features = 3\n",
    "\n",
    "data = np.column_stack((np.arange(1, 101), np.arange(1, 101), np.arange(1, 101)))\n",
    "\n",
    "# Generate random response vector, needed by staircase func\n",
    "y = np.random.normal(size=data.shape[0]).reshape(-1, 1)\n",
    "\n",
    "print(f\"Response Data Shape: {y.shape}\")\n",
    "\n",
    "# Print head of data\n",
    "print(f\"Feature Data Shape: {data.shape}\")\n",
    "data[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571efd5b-7a38-4a79-9f34-8c5c7a64933f",
   "metadata": {},
   "source": [
    "### Stateless Example\n",
    "\n",
    "When using functions that expect `batch_size` to format the data, an option is to set `batch_size` to be some large number greater than the total number of observed times $T$, so that all the data is guarenteed to be in one batch. *NOTE:* here we trick the function by using a large batch size, but `batch_size` could still be declared at the fitting stage of the model.\n",
    "\n",
    "Suppose in we use `timesteps=5`, so there are `100-5+1=96` possible ordered sequences of length `5`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfd363-be74-478a-af42-377932f2f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 5  # Number of timesteps per sequence, AKA sequence_length in args\n",
    "# batch_size = 32    # Number of samples per batch\n",
    "batch_size = int(data.shape[0]+10e6)\n",
    "print(f\"Total Observed Times: {data.shape[0]}\")\n",
    "print(f\"Batch Size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbb021-b3a2-475a-82c9-d5b9e43949a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=timeseries_dataset_from_array(\n",
    "    data,\n",
    "    targets=None,\n",
    "    sequence_length=timesteps,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "response=timeseries_dataset_from_array(\n",
    "    y,\n",
    "    targets=None,\n",
    "    sequence_length=timesteps,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f274ed-2fb8-4367-980f-d1b7ac1401e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in response:\n",
    "    print(f\"Response shape: {batch.shape}\")\n",
    "    # print(batch)\n",
    "    print(\"~\"*50)\n",
    "\n",
    "for i, batch in enumerate(dataset):\n",
    "    print(f\"Batch {i + 1} shape: {batch.shape}\")\n",
    "    print(batch)\n",
    "    print(\"~\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d3679-361b-41c7-a5c8-4e9f2bf0dd48",
   "metadata": {},
   "source": [
    "The resulting data is in the format (`n_sequence`, `timesteps`, `features`), and can be fed into a stateless RNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865805e0-caf8-4931-b9d2-6bd64cda0189",
   "metadata": {},
   "source": [
    "We now recreate this using the custom `staircase` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350c887-af78-4036-bd66-f0e9fe574d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = staircase(data, y, timesteps = 5, datapoints = len(y), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51ca33-40ea-4ef4-a26d-b1a875e61313",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14965b3-75cf-43e9-b3fe-85900e473a86",
   "metadata": {},
   "source": [
    "### Stateful Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8685b5-7e79-462b-b782-6a493dc63184",
   "metadata": {},
   "source": [
    "We now need the data in the format (`batch_size`, `timesteps`, `features`) for each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e8793-18fb-4614-ac12-92caa90c1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 5  # Number of timesteps per sequence, AKA sequence_length in args\n",
    "batch_size = 24    # Number of samples per batch\n",
    "\n",
    "dataset=timeseries_dataset_from_array(\n",
    "    data,\n",
    "    targets=None,\n",
    "    sequence_length=timesteps,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(dataset):\n",
    "    print(f\"Batch {i + 1} shape: {batch.shape}\")\n",
    "    print(batch)\n",
    "    print(\"~\"*50)\n",
    "\n",
    "print(f\"Total Batches: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341283b8-f66a-4c6b-a314-a3df4d1f08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = staircase_2(data, y, timesteps = 5, batch_size = 24, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0905891-6b67-4a47-93f7-222dedcf74cb",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/working_with_rnns#cross-batch_statefulness\n",
    "\n",
    "Tensorflow `timeseries_dataset_from_array` tutorial: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array\n",
    "\n",
    "Wiki BPTT: https://en.wikipedia.org/wiki/Backpropagation_through_time#:~:text=Backpropagation%20through%20time%20(BPTT)%20is,independently%20derived%20by%20numerous%20researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3898af7-b504-4308-87b2-54c217709a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

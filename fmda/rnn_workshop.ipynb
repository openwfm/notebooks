{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc7920-e380-4b81-bac0-cd6840450e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# Local modules\n",
    "sys.path.append('..')\n",
    "import reproducibility\n",
    "from utils import print_dict_summary\n",
    "from data_funcs import load_and_fix_data, rmse\n",
    "from moisture_rnn import RNN, create_rnn_data2\n",
    "from moisture_rnn_pkl import pkl2train\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from utils import hash2\n",
    "import copy\n",
    "import logging\n",
    "from utils import logging_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e8839-bf0e-4995-b966-c09e4df001ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298a1a1-b72c-4c7e-bcb6-2cdefe96fe3e",
   "metadata": {},
   "source": [
    "## Test Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56444dda-1e57-4b47-ad35-72ae7ed706e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"params.yaml\") as file:\n",
    "    params = yaml.safe_load(file)[\"rnn\"]\n",
    "# params.update({'scale': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666d11f-aaa2-426e-a406-70603f2799f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pkl2train(['data/reproducibility_dict2.pickle', \"data/test_CA_202401.pkl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1c7ad-6529-4c34-8444-0ef11f44dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = copy.deepcopy(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e3c25a-cf08-4a6d-be2e-21afb861c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_dat = create_rnn_data2(train['reproducibility'], params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ad412-1cae-47ad-a2c9-de9e40c30061",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "reproducibility.set_seed()\n",
    "rnn = RNN(params2)\n",
    "rnn.run_model(rnn_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a3591-e20e-42ce-b55f-32ae285e7e76",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f593ea9-40b5-46c5-a217-b41ed150bd6b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "params2.update({'val_frac': .2})\n",
    "rnn_dat = create_rnn_data2(train['CNFC1_202401'], params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1147c-e882-4b67-9e77-36a3b4b8bde4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "reproducibility.set_seed()\n",
    "rnn = RNN(params2)\n",
    "rnn.run_model(rnn_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2360aef-e9c4-4a71-922d-336e53b82537",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a73c40-0bab-4aa0-8265-00f427aa97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moisture_rnn import RNN_LSTM\n",
    "\n",
    "# from tensorflow.keras.layers import LSTM, Input, Dropout, Dense, SimpleRNN\n",
    "# from moisture_rnn import staircase_2\n",
    "# from abc import ABC, abstractmethod\n",
    "# from data_funcs import compare_dicts\n",
    "# class RNNModel(ABC):\n",
    "#     def __init__(self, params: dict):\n",
    "#         self.params = params\n",
    "#         if type(self) is RNNModel:\n",
    "#             raise TypeError(\"MLModel is an abstract class and cannot be instantiated directly\")\n",
    "#         super().__init__()\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def fit(self, X_train, y_train, weights=None):\n",
    "#         pass\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def predict(self, X):\n",
    "#         pass\n",
    "\n",
    "#     def run_model(self, dict0):\n",
    "#         # Make copy to prevent changing in place\n",
    "#         dict1 = copy.deepcopy(dict0)\n",
    "#         # Extract Fields\n",
    "#         X_train, y_train, X_test, y_test = dict1['X_train'].copy(), dict1['y_train'].copy(), dict1[\"X_test\"].copy(), dict1['y_test'].copy()\n",
    "#         if 'X_val' in dict1:\n",
    "#             X_val, y_val = dict1['X_val'].copy(), dict1['y_val'].copy()\n",
    "#         else:\n",
    "#             X_val = None\n",
    "#         case_id = dict1['case']\n",
    "\n",
    "#         # Fit model\n",
    "#         if X_val is None:\n",
    "#             self.fit(X_train, y_train)\n",
    "#         else:\n",
    "#             self.fit(X_train, y_train, validation_data = (X_val, y_val))\n",
    "#         # Generate Predictions, \n",
    "#         # run through training to get hidden state set proporly for forecast period\n",
    "#         if X_val is None:\n",
    "#             X = np.concatenate((X_train, X_test))\n",
    "#             y = np.concatenate((y_train, y_test)).flatten()\n",
    "#         else:\n",
    "#             X = np.concatenate((X_train, X_val, X_test))\n",
    "#             y = np.concatenate((y_train, y_val, y_test)).flatten()\n",
    "#         # Predict\n",
    "#         print(f\"Predicting Training through Test \\n features hash: {hash2(X)} \\n response hash: {hash2(y)} \")\n",
    "#         m = self.predict(X).flatten()\n",
    "#         dict1['m']=m\n",
    "#         dict0['m']=m # add to outside env dictionary, should be only place this happens\n",
    "#         if self.params['scale']:\n",
    "#             print(f\"Rescaling data using {self.params['scaler']}\")\n",
    "#             if self.params['scaler'] == \"reproducibility\":\n",
    "#                 m  *= self.params['scale_fm']\n",
    "#                 y  *= self.params['scale_fm']\n",
    "#                 y_train *= self.params['scale_fm']\n",
    "#                 y_test *= self.params['scale_fm']\n",
    "#         # Check Reproducibility, TODO: old dict calls it hidden_units not rnn_units, so this doens't check that\n",
    "#         if (case_id == \"reproducibility\") and compare_dicts(self.params, repro_hashes['params'], ['epochs', 'batch_size', 'scale', 'activation', 'learning_rate']):\n",
    "#             print(\"Checking Reproducibility\")\n",
    "#             checkm = m[350]\n",
    "#             hv = hash2(self.model_predict.get_weights())\n",
    "#             if self.params['phys_initialize']:\n",
    "#                 hv5 = repro_hashes['phys_initialize']['fitted_weight_hash']\n",
    "#                 mv = repro_hashes['phys_initialize']['predictions_hash']\n",
    "#             else:\n",
    "#                 hv5 = repro_hashes['rand_initialize']['fitted_weight_hash']\n",
    "#                 mv = repro_hashes['rand_initialize']['predictions_hash']           \n",
    "            \n",
    "#             print(f\"Fitted weights hash (check 5): {hv}, Reproducibility weights hash: {hv5}, Error: {hv5-hv}\")\n",
    "#             print(f\"Model predictions hash: {checkm}, Reproducibility preds hash: {mv}, Error: {mv-checkm}\")\n",
    "\n",
    "#         # print(dict1.keys())\n",
    "#         # Plot final fit and data\n",
    "#         # TODO: make plot_data specific to this context\n",
    "#         dict1['y'] = y\n",
    "#         plot_data(dict1, title=\"RNN\", title2=dict1['case'])\n",
    "        \n",
    "#         # Calculate Errors\n",
    "#         err = rmse(m, y)\n",
    "#         train_ind = dict1[\"train_ind\"] # index of final training set value\n",
    "#         test_ind = dict1[\"test_ind\"] # index of first test set value\n",
    "#         err_train = rmse(m[:train_ind], y_train.flatten())\n",
    "#         err_pred = rmse(m[test_ind:], y_test.flatten())\n",
    "#         rmse_dict = {\n",
    "#             'all': err, \n",
    "#             'training': err_train, \n",
    "#             'prediction': err_pred\n",
    "#         }\n",
    "#         return rmse_dict\n",
    "        \n",
    "# class ResetStatesCallback(Callback):\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         self.model.reset_states()\n",
    "\n",
    "\n",
    "# class RNN_LSTM(RNNModel):\n",
    "#     def __init__(self, params, loss='mean_squared_error'):\n",
    "#         super().__init__(params)\n",
    "#         self.model_train = self._build_model_train()\n",
    "#         self.model_predict = self._build_model_predict()\n",
    "\n",
    "#     def _build_model_train(self, return_sequences=False):\n",
    "#         inputs = tf.keras.Input(batch_shape=self.params['batch_shape'])\n",
    "#         x = inputs\n",
    "#         for i in range(self.params['rnn_layers']):\n",
    "#             x = LSTM(\n",
    "#                 units=self.params['rnn_units'],\n",
    "#                 activation=self.params['activation'][0],\n",
    "#                 dropout=self.params[\"dropout\"][0],\n",
    "#                 stateful=self.params['stateful'],\n",
    "#                 return_sequences=return_sequences)(x)\n",
    "#         if self.params[\"dropout\"][1] > 0:\n",
    "#             x = Dropout(self.params[\"dropout\"][1])(x)            \n",
    "#         for i in range(self.params['dense_layers']):\n",
    "#             x = Dense(self.params['dense_units'], activation=self.params['activation'][1])(x)\n",
    "#         model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=self.params['learning_rate'])\n",
    "#         model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "        \n",
    "#         if self.params[\"verbose_weights\"]:\n",
    "#             print(f\"Initial Weights Hash: {hash2(model.get_weights())}\")\n",
    "#         return model\n",
    "#     def _build_model_predict(self, return_sequences=True):\n",
    "        \n",
    "#         inputs = tf.keras.Input(shape=self.params['pred_input_shape'])\n",
    "#         x = inputs\n",
    "#         for i in range(self.params['rnn_layers']):\n",
    "#             x = LSTM(\n",
    "#                 units=self.params['rnn_units'],\n",
    "#                 activation=self.params['activation'][0],\n",
    "#                 stateful=False,return_sequences=return_sequences)(x)\n",
    "#         for i in range(self.params['dense_layers']):\n",
    "#             x = Dense(self.params['dense_units'], activation=self.params['activation'][1])(x)\n",
    "#         model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=self.params['learning_rate'])\n",
    "#         model.compile(loss='mean_squared_error', optimizer=optimizer)  \n",
    "\n",
    "#         # Set Weights to model_train\n",
    "#         w_fitted = self.model_train.get_weights()\n",
    "#         model.set_weights(w_fitted)\n",
    "        \n",
    "#         return model\n",
    "#     def format_train_data(self, X, y, verbose=False):\n",
    "#         X, y = staircase_2(X, y, timesteps = self.params[\"timesteps\"], batch_size=self.params[\"batch_size\"], verbose=verbose)\n",
    "#         return X, y\n",
    "#     def format_pred_data(self, X):\n",
    "#         return np.reshape(X,(1, X.shape[0], self.params['features']))\n",
    "#     def fit(self, X_train, y_train, plot=True, plot_title = '', \n",
    "#             weights=None, callbacks=[], verbose_fit=None, validation_data=None, *args, **kwargs):\n",
    "#         # verbose_fit argument is for printing out update after each epoch, which gets very long\n",
    "#         # These print statements at the top could be turned off with a verbose argument, but then\n",
    "#         # there would be a bunch of different verbose params\n",
    "#         print(f\"Training simple RNN with params: {self.params}\")\n",
    "#         X_train, y_train = self.format_train_data(X_train, y_train)\n",
    "#         print(f\"X_train hash: {hash2(X_train)}\")\n",
    "#         print(f\"y_train hash: {hash2(y_train)}\")\n",
    "#         if validation_data is not None:\n",
    "#             X_val, y_val = self.format_train_data(validation_data[0], validation_data[1])\n",
    "#             print(f\"X_val hash: {hash2(X_val)}\")\n",
    "#             print(f\"y_val hash: {hash2(y_val)}\")\n",
    "#         print(f\"Initial weights before training hash: {hash2(self.model_train.get_weights())}\")\n",
    "#         # Setup callbacks\n",
    "#         if self.params[\"reset_states\"]:\n",
    "#             callbacks=callbacks+[ResetStatesCallback()]\n",
    "        \n",
    "#         # Note: we overload the params here so that verbose_fit can be easily turned on/off at the .fit call \n",
    "#         if verbose_fit is None:\n",
    "#             verbose_fit = self.params['verbose_fit']\n",
    "#         # Evaluate Model once to set nonzero initial state\n",
    "#         if self.params[\"batch_size\"]>= X_train.shape[0]:\n",
    "#             self.model_train(X_train)\n",
    "#         if validation_data is not None:\n",
    "#             history = self.model_train.fit(\n",
    "#                 X_train, y_train+self.params['centering'][1], \n",
    "#                 epochs=self.params['epochs'], \n",
    "#                 batch_size=self.params['batch_size'],\n",
    "#                 callbacks = callbacks,\n",
    "#                 verbose=verbose_fit,\n",
    "#                 validation_data = (X_val, y_val),\n",
    "#                 *args, **kwargs\n",
    "#             )\n",
    "#         else:\n",
    "#             history = self.model_train.fit(\n",
    "#                 X_train, y_train+self.params['centering'][1], \n",
    "#                 epochs=self.params['epochs'], \n",
    "#                 batch_size=self.params['batch_size'],\n",
    "#                 callbacks = callbacks,\n",
    "#                 verbose=verbose_fit,\n",
    "#                 *args, **kwargs\n",
    "#             )\n",
    "#         if plot:\n",
    "#             self.plot_history(history,plot_title)\n",
    "#         if self.params[\"verbose_weights\"]:\n",
    "#             print(f\"Fitted Weights Hash: {hash2(self.model_train.get_weights())}\")\n",
    "\n",
    "#         # Update Weights for Prediction Model\n",
    "#         w_fitted = self.model_train.get_weights()\n",
    "#         self.model_predict.set_weights(w_fitted)\n",
    "#     def predict(self, X_test):\n",
    "#         print(\"Predicting with simple RNN\")\n",
    "#         X_test = self.format_pred_data(X_test)\n",
    "#         preds = self.model_predict.predict(X_test).flatten()\n",
    "#         return preds\n",
    "\n",
    "\n",
    "#     def plot_history(self, history, plot_title):\n",
    "#         plt.semilogy(history.history['loss'], label='Training loss')\n",
    "#         if 'val_loss' in history.history:\n",
    "#             plt.semilogy(history.history['val_loss'], label='Validation loss')\n",
    "#         plt.title(f'{plot_title} Model loss')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.legend(loc='upper left')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3ee62-4bef-4eb9-a599-405beaa0632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_dat = create_rnn_data2(train['reproducibility'],params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b05a3-8788-4fcb-9a8b-3c8c28426803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import LSTM, Input, Dropout, Dense\n",
    "reproducibility.set_seed()\n",
    "params2.update({'epochs': 50})\n",
    "lstm = RNN_LSTM(params2)\n",
    "lstm.fit(rnn_dat[\"X_train\"], rnn_dat[\"y_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51fbc8c-9b0f-42c9-bfbb-5f4c0cacaa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = RNN_LSTM(params2)\n",
    "lstm.fit(rnn_dat[\"X_train\"], rnn_dat[\"y_train\"], \n",
    "         validation_data = (rnn_dat['X_val'], rnn_dat['y_val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df67d38-fbe0-4e3b-9dd9-ec25d8378e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from moisture_rnn import repro_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ecf9fd-d026-4edf-91ba-6fd050dbd1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = RNN_LSTM(params2)\n",
    "lstm.run_model(rnn_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4e441-9bf1-4d57-bb37-091553e23212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib \n",
    "import moisture_rnn\n",
    "importlib.reload(moisture_rnn)\n",
    "from moisture_rnn import RNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59480f19-3567-4b24-b6ff-d9292dc8c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"params.yaml\") as file:\n",
    "    params = yaml.safe_load(file)[\"lstm\"]\n",
    "    \n",
    "rnn_dat = create_rnn_data2(train['reproducibility'],params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adff592-7aa4-4e59-a229-cad4a133297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7ae31-e3fb-4a44-95bd-093ec34e0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reproducibility.set_seed()\n",
    "lstm = RNN_LSTM(params)\n",
    "lstm.fit(rnn_dat[\"X_train\"], rnn_dat[\"y_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9d612e-8cd2-40ca-a789-91c99c3d6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({'epochs': 75})\n",
    "reproducibility.set_seed()\n",
    "lstm = RNN_LSTM(params)\n",
    "lstm.run_model(rnn_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a24c6-9a67-45aa-bc5c-8818aa0ca049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

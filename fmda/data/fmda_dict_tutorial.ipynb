{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c089ba-5037-43e0-82ca-a5f8dcd4c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import os\n",
    "import os.path as osp\n",
    "from osgeo import gdal, osr\n",
    "from scipy.interpolate import griddata, RegularGridInterpolator\n",
    "from synoptic.services import stations_timeseries, stations_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3fd0f-d4dc-4d3d-ab91-77bd19e4b64b",
   "metadata": {},
   "source": [
    "# FMDA Dictionary Tutorial\n",
    "\n",
    "The purpose of this notebook is to demonstrate creating fmda dictionaries to be used for training ML models of fuel moisture. This notebook combines the techniques from other notebooks in this directory, so see `interpolation_tutorial` and `synopticpy_tutorial` for more information. This code will live in `wrfxpy` in the python module `build_fmda_dict.py`.\n",
    "\n",
    "**Goals:** given a user input of a date range and latitude/longitude bounding box, return a dictionary with top-level keys for each RAWS station within the bounding box that has fuel moisture data, and then for each station subdictionaries of formatted static location information, RAWS goundlevel sensor data, and atmospheric data from HRRR interpolated to the station location.\n",
    "\n",
    "This notebook will demonstrate retrieving RAWS data using `SynopticPy`, but within `wrfxpy` for older times this data is retrieved from a stash of saved fuel moisture data.\n",
    "\n",
    "*NOTE:* this requires a formatted stash of geotiff files, which are bands extracted from HRRR grib files.\n",
    "\n",
    "*NOTE:* need to reconcile rain units between RAWS and HRRR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ea125-109c-407a-a9b2-656ceae7604c",
   "metadata": {},
   "source": [
    "## User Inputs\n",
    "\n",
    "Below we manually enter the user inputs to define the spatiotemporal frame for the data collection. Within `wrfxpy`, these arguments are entered from the command line and read within python as system arguments with `sys.argv[...]`. The arguments should be formatted as:\n",
    "\n",
    "* `start`: (str) start time formatted as \"YYYYmmDDHHMM\"\n",
    "* `end`: (str) end time formatted as \"YYYYmmDDHHMM\"\n",
    "* `bbox`: (list) of format `[lonmin, latmin, lonmax, lonmin]` (mimicking format from `SynopticPy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6f8bb-7cd0-4a36-9c9d-2f26da4c0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"202401010000\" # Jan 1, 2024, midnight UTC\n",
    "end = \"202401010200\" # Jan 31, 2024, 2am UTC\n",
    "bbox = [-105, 37, -103, 39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35fb339-6cd3-4df4-a049-2ad3b3246b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format times as datetime\n",
    "t0 = datetime.strptime(start, \"%Y%m%d%H%M\")\n",
    "t1 = datetime.strptime(end, \"%Y%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c4812-a346-4501-b34d-a6b8ad4f40bb",
   "metadata": {},
   "source": [
    "## Static Data Objects\n",
    "\n",
    "Below are objects decalred at the start of `build_fmda_dict.py` and used throughout. They include a dataframe of HRRR data bands, determined from [HRRR documentation](https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfprsf00.grib2.shtml). Also, there is a file path string object `hrrrpath` which points to the stash of formatted geotiff files. For this tutorial, those data simply live in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca7ce6-092f-4a99-931f-626e46f2d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: choosing to exclude solar bands 'DLWRF', 'USWRF', 'ULWRF'\n",
    "# Downward shortwave is expected theoretically to be the most useful solar field \n",
    "# RAWS have downward shortwave sensors, so these could be compared to model fields\n",
    "band_df_hrrr = pd.DataFrame({\n",
    "    'Band': [585, 616, 620, 628, 629, 661],\n",
    "    'hrrr_name': ['GUST', 'TMP', 'RH', 'PRATE', 'APCP',\n",
    "                  'DSWRF'],\n",
    "    'dict_name': [\"wind\", \"temp\", \"rh\", \"rain\", \"precip_accum\",\n",
    "                 \"solar\"],\n",
    "    'descr': ['surface Wind Speed (Gust) [m/s]',\n",
    "              '2 m Temperature [K]', \n",
    "              '2 m Relative Humidity [%]', \n",
    "              'surface Precip. Rate [kg/m^2/s]',\n",
    "              'surface Total Precipitation [kg/m^2]',\n",
    "              'surface Downward Short-Wave Radiation Flux [W/m^2]']\n",
    "})\n",
    "\n",
    "hrrrpath = \"geotiff_files\" # path for atmospheric data stash\n",
    "\n",
    "utc_format = \"%Y-%m-%dT%H:%M:%SZ\" # date format for UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98974707-72fe-4205-a7b6-593b89e3a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Building FMDA Dictionary for RAWS Sites within {bbox}, from {t0} to {t1}\")\n",
    "print(\"~\"*50)\n",
    "band_df_hrrr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab6ed9-6a77-447d-913d-e235a429f6f0",
   "metadata": {},
   "source": [
    "## Get RAWS Station Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6ca0d-b5a1-4a9a-8997-a86aab96bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = stations_metadata(bbox=bbox,vars=[\"fuel_moisture\"])\n",
    "print(f\"Number of Stations within bbox: {sts.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3ef7c-8066-44c4-a67f-61392da31608",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70544efd-02e7-4a12-ae0a-72f5f5f820a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    stid=[\"PLACEHOLDER\"], # change this in the loop\n",
    "    vars=[\"air_temp\", \"relative_humidity\", \"precip_accum\", \"fuel_moisture\", \"wind_speed\", \"solar_radiation\"],\n",
    "    start=t0,\n",
    "    end= t1+timedelta(hours=1) # add an hour since it doesn't include end date exactly\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651eb955-9768-4841-b0a6-16c5b58ee8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_precip(precipa):\n",
    "    rain=np.array(precipa, dtype = 'float64')\n",
    "    rain = np.diff(rain) # first difference to convert accumulated to hourly\n",
    "    rain = np.insert(rain, 0, [np.NaN]) # add NaN entry to account for diff\n",
    "    # Highest ever recorded hourly rainfall in inches is about 16: https://www.weather.gov/owp/hdsc_world_record\n",
    "    rain[rain > 100] = np.NaN # filter out erroneously high\n",
    "    rain[rain < 0] = np.NaN # filter out negative, results from diff function after precipa goes to zero\n",
    "    return rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d42b9-8c32-408f-b171-931eed01b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_raws_df(df, tstart, tend):\n",
    "    # Given input dataframe (the output of retrieve_raws_api), return formatted dictionary\n",
    "    # Inputs:\n",
    "    # df: (dataframe)\n",
    "    # Returns: fmda dictionary\n",
    "\n",
    "    ## Format Return Dictionaries\n",
    "    loc = {\n",
    "        \"STID\": df.attrs[\"STID\"],\n",
    "        'lat' : df.attrs['latitude'],\n",
    "        'lon' : df.attrs['longitude'],\n",
    "        'elev': df.attrs[\"ELEVATION\"]\n",
    "        'descr': \"Sources: lat, lon, and elev from synoptic.services.stations_timeseries\"\n",
    "    }\n",
    "    \n",
    "    ## Extract times from dataframe index\n",
    "    times = df.index.strftime('%Y-%m-%dT%H:%M:%SZ').to_numpy() # convert index to utc time\n",
    "    ## Convert dataframe to dictionary\n",
    "    raws = df.to_dict(orient = \"list\")\n",
    "    \n",
    "    # Convert lists to NumPy arrays\n",
    "    raws = {key: np.array(value) for key, value in raws.items()}\n",
    "\n",
    "    raws[\"time_raws\"]=times\n",
    "    raws[\"hours\"]=len(times)\n",
    "    \n",
    "    ## Convert C to K \n",
    "    if df.attrs[\"UNITS\"][\"air_temp\"] == \"Celsius\":\n",
    "        print(\"Converting RAWS temp from C to K\")\n",
    "        raws[\"air_temp\"] = raws[\"air_temp\"]+273.15\n",
    "\n",
    "    ## Calculate Hourly Precipitation from accumulated\n",
    "    if \"precip_accum\" in df.columns:\n",
    "        print(\"Calculating hourly precipitation\")\n",
    "        raws[\"rain\"] = format_precip(raws[\"precip_accum\"])\n",
    "\n",
    "    ## Format Names\n",
    "    name_mapping = {\"air_temp\":\"temp\", \"fuel_moisture\":\"fm\", \"relative_humidity\":\"rh\", \"precip_accum\":\"rain\",\"solar_radiation\":\"solar\", \"wind_speed\":\"wind\", \"precip_accum\":\"precip_accum\"}\n",
    "    old_keys = [*raws.keys()]\n",
    "    new_keys = []\n",
    "    for key in old_keys:\n",
    "        new_keys.append(name_mapping.get(key, key))\n",
    "    old_keys = [*raws.keys()]\n",
    "    new_keys = []\n",
    "    for key in old_keys:\n",
    "        new_keys.append(name_mapping.get(key, key))\n",
    "    raws = dict(zip(new_keys, list(raws.values())))\n",
    "\n",
    "    ## Add array of times requested, often different from returned time by a couple mins\n",
    "    times = pd.date_range(start=tstart,end=tend, freq=\"1H\")\n",
    "    raws[\"time\"]=times.strftime('%Y-%m-%dT%H:%M:%SZ').to_numpy()\n",
    "\n",
    "    ## Calculate Equilibria if available\n",
    "    if {\"rh\", \"temp\"} & set(raws.keys()):\n",
    "        print(\"Calculating Equilibrium Moisture from RAWS data\")\n",
    "        raws['Ed'] = 0.924*raws['rh']**0.679 + 0.000499*np.exp(0.1*raws['rh']) + 0.18*(21.1 + 273.15 - raws['temp'])*(1 - np.exp(-0.115*raws['rh']))\n",
    "        raws['Ew'] = 0.618*raws['rh']**0.753 + 0.000454*np.exp(0.1*raws['rh']) + 0.18*(21.1 + 273.15 - raws['temp'])*(1 - np.exp(-0.115*raws['rh']))\n",
    "    \n",
    "    return loc, raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760ab95-217e-4c77-8f32-2f784be544b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return nested dictionary, \n",
    "# Top-level keys is station ID with start YYYYmm\n",
    "# Next-level keys is location data and RAWS sensor data\n",
    "def build_raws_dict(sts, tstart, tend):\n",
    "    # Inputs:\n",
    "    # sts: (df) dataframe of station data, output of stations_metadata\n",
    "    out_dict = {} # set up return dictionary\n",
    "\n",
    "    for st in sts:\n",
    "        print(\"~\"*50)\n",
    "        print(f\"Collecting RAWS data for {st}\")\n",
    "        params[\"stid\"] = [st]\n",
    "        try:\n",
    "            dat = stations_timeseries(verbose=\"HIDE\", **params)\n",
    "    \n",
    "            if \"fuel_moisture\" in dat.columns:\n",
    "                print(\"Collected FMC data\")\n",
    "                loc, raws = format_raws_df(dat,tstart, tend)\n",
    "                title = f\"{st}_{t0.year}{t0.strftime('%m')}\"\n",
    "                out_dict[title] = {\"loc\":loc, \"RAWS\": raws}\n",
    "            else:\n",
    "                print(\"No FMC found for this station and time\")\n",
    "        except AssertionError as e:\n",
    "            # Error handling behavior\n",
    "            print(\"AssertionError caught:\", e)\n",
    "            \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004f467-fed4-4c25-a892-f6573fa7442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = build_raws_dict(sts, t0, t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef10c9-54e9-4c3a-86b9-67d7f0aca83f",
   "metadata": {},
   "source": [
    "### View collected data after this step\n",
    "\n",
    "*Note:* this is for illustration only, not done within `wrfxpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da7433-4737-4fa9-a43f-e1a2cffe5fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c910c4b-a145-4428-af72-dc87097ae812",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict[\"CCEC2_202401\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368193ec-a56e-4bbb-8335-f52e8507e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict[\"CCEC2_202401\"][\"loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283021ee-06f5-42c0-9c9c-8916b1aa1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict[\"CCEC2_202401\"][\"RAWS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9414b2-8976-4e6d-86d1-246fc5d76973",
   "metadata": {},
   "source": [
    "## Get HRRR Data\n",
    "\n",
    "Using dictionary produced by RAWS data retrieval above, fill with time series of interpolated HRRR data at each location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c863cc-d7f6-46b2-b4fe-18681c70b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projection_info(ds, epsg = 4326):\n",
    "    # Given a geotiff file (a HRRR band), \n",
    "    # return info necessary to transform lat/lon coords to the file structure\n",
    "    # Inputs: \n",
    "    # ds: (osgeo.gdal.Dataset)\n",
    "    # epsg: (int) default 4326 for lon/lat\n",
    "    # Return: (tuple) with fields (ct, g_inv)\n",
    "        # ct: (osgeo.osr.CoordinateTransformation)\n",
    "        # gt_inv: (tuple) output of gdal.InvGeoTransform, also could be found with gdalinfo on command line\n",
    "    gt = ds.GetGeoTransform()\n",
    "    gp = ds.GetProjection()\n",
    "    if(ds.RasterCount>1):\n",
    "        print('Not Implemented for multiple Raster bands')\n",
    "        sys.exit(-1)\n",
    "    # Get Projection info\n",
    "    point_srs = osr.SpatialReference()\n",
    "    point_srs.ImportFromEPSG(4326) # hardcode for lon/lat\n",
    "    # GDAL>=3: make sure it's x/y\n",
    "    # see https://trac.osgeo.org/gdal/wiki/rfc73_proj6_wkt2_srsbarn\n",
    "    point_srs.SetAxisMappingStrategy(osr.OAMS_TRADITIONAL_GIS_ORDER)\n",
    "    file_srs = osr.SpatialReference()\n",
    "    file_srs.ImportFromWkt(gp)\n",
    "    ct = osr.CoordinateTransformation(point_srs, file_srs)\n",
    "    gt_inv = gdal.InvGeoTransform(gt)\n",
    "\n",
    "    return ct, gt_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88b1c8-3355-4902-b066-f38a10e9f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hrrr_path(d, band):\n",
    "    # Inputs: \n",
    "    # d: (datetime)\n",
    "    # band: (int) HRRR band number\n",
    "    # Returns: (str) filepath to geotiff file\n",
    "    day_file = d.strftime(\"%Y%m%d\") # HRRR data stash is in this format\n",
    "    hour = d.strftime(\"%H\")\n",
    "    tpath = osp.join(hrrrpath, day_file, f\"hrrr.t{hour}z.wrfprsf00.{band}.tif\")\n",
    "    return tpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81afdee1-0ead-429e-adba-ee286406f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_at(interp_x, interp_y, values, method = \"linear\"):\n",
    "    # Python implementation on regular grid of Jan methodology from https://github.com/openwfm/wrf-fire-matlab/blob/master/vis/ts_at.m\n",
    "    interp_pts = np.array([interp_y, interp_x])\n",
    "\n",
    "    # Get nearest neighbor\n",
    "    center_x = round(interp_x)\n",
    "    center_y = round(interp_y)\n",
    "\n",
    "    # Build 3x3 grid around center, NOTE: xy flip in GDAL\n",
    "    grid = np.meshgrid(np.array([center_y-1, center_y, center_y+1]),\n",
    "            np.array([center_x-1, center_x, center_x+1]))\n",
    "    grid = np.array([grid[0].flatten(), grid[1].flatten()]).T\n",
    "    # Subset values\n",
    "    value9 = values[\n",
    "        grid[:,0],\n",
    "        grid[:,1]]\n",
    "    value9=value9.reshape(3,3)\n",
    "\n",
    "    # print(f\"Using method: {method}\")\n",
    "\n",
    "    interp = RegularGridInterpolator([np.array([center_y-1, center_y, center_y+1]),np.array([center_x-1, center_x, center_x+1])], value9)\n",
    "\n",
    "    return interp(interp_pts, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231371c-12b5-432a-b836-9c87a7956738",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_df_hrrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6dc2e1-9470-4978-8395-13f4860ab20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hrrr_dict(tstart, tend, dat, method = 'linear'):\n",
    "    # tstart: (datetime)    start time\n",
    "    # tend: (datetime)     end time\n",
    "    # dat: (dict) dictionary, output of build_raws_dict\n",
    "    # method: (str) interpolation method, passed to ts_at\n",
    "    \n",
    "    # Get times array\n",
    "    times = pd.date_range(start=tstart,end=tend, freq=\"1H\")\n",
    "\n",
    "    # Get Projection data from first band from band_df_hrrr,\n",
    "    # reuse projection info for other bands\n",
    "    # NOTE: this results in 1 extra read of geotiff files, but doing it for clarity\n",
    "    d = times[0]\n",
    "    band = band_df_hrrr.Band[0]\n",
    "    tpath = build_hrrr_path(d, band)\n",
    "    print(f\"Opening: {tpath}\")\n",
    "    if not osp.exists(tpath): \n",
    "        raise FileNotFoundError(f\"The file '{tpath}' does not exist.\")\n",
    "    ds = gdal.Open(tpath)\n",
    "    ct, gt_inv = get_projection_info(ds)\n",
    "    ds = None # close connection\n",
    "    print(f\"Projection info collected: {gt_inv}\")\n",
    "    \n",
    "    # Set up dictionary entries and projected pixel values\n",
    "    # Format HRRR subdictionary with key for each band in band_df_hrrr, np.nan as placeholder\n",
    "    # Add pixel_x and pixel_y to loc subdirectory to use with interpolation\n",
    "    for k in dat.keys():\n",
    "        dat[k][\"HRRR\"] = {\"time\": times.strftime(utc_format).to_numpy()} # Initialize HRRR subdir with times\n",
    "        for name in band_df_hrrr.dict_name:\n",
    "            dat[k][\"HRRR\"][name] = np.full(len(times), np.nan, dtype=np.float64) # Initialize time series with np.nan\n",
    "        lon = dat[k][\"loc\"][\"lon\"]\n",
    "        lat = dat[k][\"loc\"][\"lat\"]\n",
    "        print(f\"Interpolating to RAWS {dat[k]['loc']['STID']}, target lat/lon {lat, lon}\")\n",
    "        mapx, mapy, z = ct.TransformPoint(np.float64(lon), np.float64(lat))\n",
    "        pixel_x, pixel_y = gdal.ApplyGeoTransform(gt_inv, mapx, mapy)\n",
    "        print(f\"Projected pixel: ({pixel_x}, {pixel_y})\")\n",
    "        dat[k][\"loc\"][\"pixel_x\"] = pixel_x\n",
    "        dat[k][\"loc\"][\"pixel_y\"] = pixel_y\n",
    "        print(\"\")\n",
    "    \n",
    "    # Loop over bands, build time series for each station in dictionary\n",
    "    for index, row in band_df_hrrr.iterrows():\n",
    "        print(\"~\"*50)\n",
    "        band = row[\"Band\"]\n",
    "        dict_name = row[\"dict_name\"]\n",
    "        print(f\"Building Time Series for band: {band}, {row['descr']}\")\n",
    "        for i in range(0, len(times)):\n",
    "            d = times[i]\n",
    "            print(f\"Collecting Data for date: {d}\")\n",
    "            tpath = build_hrrr_path(d, band)\n",
    "            print(f\"Opening: {tpath}\")\n",
    "            ds = gdal.Open(tpath)\n",
    "            data = ds.GetRasterBand(1).ReadAsArray()\n",
    "            # Loop over dictionary keys to interpolte for each station loc\n",
    "            for k in dat:\n",
    "                pixel_x = dat[k][\"loc\"][\"pixel_x\"]\n",
    "                pixel_y = dat[k][\"loc\"][\"pixel_y\"]\n",
    "                interp_val = ts_at(pixel_x, pixel_y, data, method = method)\n",
    "                # Fill appropriate array value with interpolated value\n",
    "                dat[k][\"HRRR\"][dict_name][i] = interp_val\n",
    "            ds = None # close connection\n",
    "\n",
    "    # Loop over keys to:\n",
    "    # Convert temp C to K, Calculate Equilibria Moisture from HRRR Data, add source description\n",
    "    # NOTE: this assumes simple celcius temps, which is what we have seen in HRRR \n",
    "        # But weather data may come in other formats\n",
    "        # we should check the original grib files gdalinfo for temp being units C within the code\n",
    "    print(\"~\"*50)\n",
    "    print(\"Calculating moisture Equilibria from rh and temp\")\n",
    "    for k in dat:\n",
    "        rh = dat[k][\"HRRR\"][\"rh\"]\n",
    "        temp = dat[k][\"HRRR\"][\"temp\"]\n",
    "        if np.any(temp < 150):\n",
    "            temp += 273.15\n",
    "            # Below if statement to only print once\n",
    "            if k == [*out_dict.keys()][0]:\n",
    "                print(\"Converting HRRR data temp from C to K\") \n",
    "        dat[k][\"HRRR\"][\"Ed\"] = 0.924*rh**0.679 + 0.000499*np.exp(0.1*rh) + 0.18*(21.1 + 273.15 - temp)*(1 - np.exp(-0.115*rh))\n",
    "        dat[k][\"HRRR\"][\"Ew\"] = 0.618*rh**0.753 + 0.000454*np.exp(0.1*rh) + 0.18*(21.1 + 273.15 - temp)*(1 - np.exp(-0.115*rh))    \n",
    "        dat[k][\"HRRR\"][\"descr\"] = f\"Source: HRRR data from 3d pressure model, linear grid interpolated to RAWS location\"\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f71b2-27c1-4cc4-a3b1-1ddd9eafda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"~\"*50)\n",
    "out_dict = build_hrrr_dict(t0, t1, out_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42926777-e16d-4ff7-ab33-edf46f47a695",
   "metadata": {},
   "source": [
    "## Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5da39-eb1a-4632-9331-374b7fda33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level keys\n",
    "out_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8209b-f62b-4c4f-9568-e5f4d34ccdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Level Keys (for first entry of top level)\n",
    "out_dict[\"CCEC2_202401\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb263b-12ed-4f0b-b036-cc15135c1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location Data (for first entry of top level)\n",
    "out_dict[\"CCEC2_202401\"][\"loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105765b-cf2a-43e6-b181-d6d4238b8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAWS Data (for first entry of top level)\n",
    "out_dict[\"CCEC2_202401\"][\"RAWS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d79d77-3aea-486f-aa2d-a612ee35b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HRRR Data (for first entry of top level)\n",
    "out_dict[\"CCEC2_202401\"][\"HRRR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daba36b-ee22-4c4c-b9fb-92476a2589be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# File used to store hyperparameters.

# Stateless RNN, batch_size declared at fit step
rnn:
  batch_size: 32
  timesteps: 5
  optimizer: adam
  rnn_layers: 1 # Number of hidden recurrent layers
  rnn_units: 20 # Number of units per hidden recurrent layer
  dense_layers: 1 # hidden dense layers AFTER recurrent layers and BEFORE final output cell
  dense_units: 5 # number of units for hidden dense layers
  activation: ['linear', 'linear'] # Activation type for hidden layers, dense layers respectively
  centering: [0.0,0.0]
  dropout: [0.2, 0.2]  # NOTE: length must match total number of layers, default is 1 hidden recurrent layer and 1 dense output layer
  recurrent_dropout: 0.2 # Length must match number of recurrent layers
  reset_states: True # reset hidden states after training epoch, triggers reset_states() via callbacks
  # batch_reset: 10 # reset states after given number of batches
  batch_schedule_type: 'constant' # Schedule to Reset Hidden State
  bmin: 20 # Minimum number of batches for batch reset schedule
  epochs: 20
  learning_rate: 0.001
  clipvalue: 10.0 # gradient clipping param, gradient can't exceed this value
  phys_initialize: False # physics initialization
  stateful: True
  verbose_weights: True # Prints out hashs of weights for tracking reproducibility
  verbose_fit: False # Prints out all training epochs, makes computation much slower
  # features_list: ['Ed', 'Ew', 'solar', 'wind', 'rain']
  features_list: ['Ed', 'Ew', 'rain']
  scale: True
  scaler: 'standard' # One of methods in scalers dictionary in moisture_rnn.py
  train_frac: 0.5 # percent of input data to be used in training set
  val_frac: 0.1 # percent of input data to be used in validation set. Test set size determined from train_frac and val_frac
  early_stopping_patience: 5 # Number of epochs with no improvement after which training will be stopped.
  

lstm:
  batch_size: 32
  timesteps: 5
  optimizer: adam
  rnn_layers: 1
  rnn_units: 6
  dense_layers: 1
  dense_units: 1
  activation: ['linear', 'linear']
  recurrent_activation: 'sigmoid'
  centering: [0.0,0.0]
  dropout: [0.2, 0.2]  # NOTE: length must match total number of layers, default is 1 hidden recurrent layer and 1 dense output layer
  recurrent_dropout: 0.2 # Length must match number of recurrent layers
  reset_states: True # reset hidden states after training epoch, triggers reset_states() via callbacks
  # batch_reset: 150 # reset states after given number of batches  
  epochs: 100
  learning_rate: 0.0001
  clipvalue: 1.0 # gradient clipping param, gradient can't exceed this value
  phys_initialize: False # physics initialization
  stateful: True
  verbose_weights: True # Prints out hashs of weights for tracking reproducibility
  verbose_fit: False # Prints out all training epochs, makes computation much slower
  features_list: ['Ed', 'Ew', 'rain']
  scale: True
  scaler: 'minmax' # One of methods in scalers dictionary in moisture_rnn.py
  train_frac: 0.5
  val_frac: 0.1
  early_stopping_patience: 5 # Number of epochs with no improvement after which training will be stopped.


# Param sets for reproducibility

## v2.1 params
rnn_repro:
  batch_size: 32
  timesteps: 5
  optimizer: adam
  rnn_layers: 1
  rnn_units: 20
  dense_layers: 1
  dense_units: 5
  activation: ['linear', 'linear']
  centering: [0.0, 0.0]
  dropout: [0.2, 0.2]
  recurrent_dropout: 0.2
  reset_states: True
  # batch_reset: null
  epochs: 300
  learning_rate: 0.001
  clipvalue: 10.0
  phys_initialize: False
  stateful: True
  verbose_weights: True
  verbose_fit: False
  features_list: ['Ed', 'Ew', 'solar', 'wind', 'rain']
  scale: True
  scaler: 'minmax'
  train_frac: 0.5
  val_frac: 0.2
  early_stopping_patience: 9999 # early stopping not used in repro case, so setting to a huge value to ignore

## v2.0 params
# rnn_repro:
#   batch_size: 32
#   timesteps: 5
#   optimizer: adam
#   epochs: 200
#   rnn_layers: 1
#   rnn_units: 20
#   dense_layers: 0 # hidden dense layers AFTER recurrent layers and BEFORE final output cell
#   dense_units: 1
#   activation: ['linear', 'linear']
#   centering: [0.0,0.0]
#   dropout: [0.0, 0.0]  # NOTE: length must match total number of layers, default is 1 hidden recurrent layer and 1 dense output layer
#   recurrent_dropout: 0.0 # Length must match number of recurrent layers
#   reset_states: True # reset hidden states after training epoch, triggers reset_states() via callbacks
#   epochs: 200
#   learning_rate: 0.001
#   phys_initialize: False # physics initialization
#   stateful: True
#   verbose_weights: True # Prints out hashs of weights for tracking reproducibility
#   verbose_fit: False # Prints out all training epochs, makes computation much slower
#   features_list: ['Ed', 'Ew', 'rain']
#   scale: True
#   scaler: 'reproducibility'
#   train_frac: 0.5
#   val_frac: 0.0



  
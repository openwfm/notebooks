# File used to store hyperparameters.

# RNN Params
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Stateless RNN, batch_size declared at fit step
rnn:
  batch_size: 32
  timesteps: 12
  optimizer: adam
  rnn_layers: 1 # Number of hidden recurrent layers
  rnn_units: 20 # Number of units per hidden recurrent layer
  dense_layers: 1 # hidden dense layers AFTER recurrent layers and BEFORE final output cell
  dense_units: 5 # number of units for hidden dense layers
  activation: ['relu', 'relu'] # Activation type for hidden layers, dense layers respectively
  dropout: [0.2, 0.2]  # NOTE: length must match total number of layers, default is 1 hidden recurrent layer and 1 dense output layer
  recurrent_dropout: 0.2 # Length must match number of recurrent layers
  reset_states: True # reset hidden states after training epoch, triggers reset_states() via callbacks
  # batch_reset: 10 # reset states after given number of batches
  batch_schedule_type: 'exp' # Schedule to Reset Hidden State
  bmin: 20 # Minimum number of batches for batch reset schedule
  bmax: 200
  epochs: 20
  learning_rate: 0.001
  clipvalue: 10.0 # gradient clipping param, gradient can't exceed this value
  phys_initialize: False # physics initialization
  stateful: True
  verbose_weights: True # Prints out hashs of weights for tracking reproducibility
  verbose_fit: False # Prints out all training epochs, makes computation much slower
  # features_list: ['Ed', 'Ew', 'solar', 'wind', 'rain']
  features_list: ['Ed', 'Ew', 'rain']
  scale: True
  scaler: 'standard' # One of methods in scalers dictionary in moisture_rnn.py
  time_fracs: [.9, .05, .05] # Percentage of data based on time span for train/val/test
  early_stopping_patience: 5 # Number of epochs with no improvement after which training will be stopped.
  predict_spinup_hours: 5 # Number of hours to run through the model before prediction errors evaluated. Used to stabilize hidden state

lstm:
  batch_size: 32
  timesteps: 12
  optimizer: adam
  rnn_layers: 1
  rnn_units: 6
  dense_layers: 1
  dense_units: 1
  activation: ['relu', 'relu']
  recurrent_activation: 'sigmoid'
  dropout: [0.2, 0.2]  # NOTE: length must match total number of layers, default is 1 hidden recurrent layer and 1 dense output layer
  recurrent_dropout: 0.2 # Length must match number of recurrent layers
  reset_states: True # reset hidden states after training epoch, triggers reset_states() via callbacks
  batch_schedule_type: 'constant' # Schedule to Reset Hidden State
  bmin: 20 # Minimum number of batches for batch reset schedule
  bmax: 200
  epochs: 100
  learning_rate: 0.0001
  clipvalue: 1.0 # gradient clipping param, gradient can't exceed this value
  phys_initialize: False # physics initialization
  stateful: True
  verbose_weights: True # Prints out hashs of weights for tracking reproducibility
  verbose_fit: False # Prints out all training epochs, makes computation much slower
  features_list: ['Ed', 'Ew', 'rain']
  scale: True
  scaler: 'minmax' # One of methods in scalers dictionary in moisture_rnn.py
  time_fracs: [.9, .05, .05] # Percentage of data based on time span for train/val/test
  early_stopping_patience: 25 # Number of epochs with no improvement after which training will be stopped.
  predict_spinup_hours: 5 # Number of hours to run through the model before prediction errors evaluated. Used to stabilize hidden state

# Param sets for reproducibility

## v2.1 params
rnn_repro:
  batch_size: 32
  timesteps: 5
  optimizer: adam
  rnn_layers: 1
  rnn_units: 20
  dense_layers: 1
  dense_units: 5
  activation: ['linear', 'linear']
  dropout: [0.2, 0.2]
  recurrent_dropout: 0.2
  reset_states: True
  epochs: 300
  learning_rate: 0.001
  clipvalue: 10.0
  phys_initialize: False
  stateful: True
  verbose_weights: True
  verbose_fit: False
  features_list: ['Ed', 'Ew', 'solar', 'wind', 'rain']
  scale: True
  scaler: 'minmax'
  time_fracs: [.5, .2, .3] # Percentage of data based on time span for train/val/test
  early_stopping_patience: 9999 # early stopping not used in repro case, so setting to a huge value to ignore
  predict_spinup_hours: null # Number of hours to run through the model before prediction errors evaluated. Used to stabilize hidden state
  
## v2.0 params
# rnn_repro:
#   batch_size: 32
#   timesteps: 5
#   optimizer: adam
#   epochs: 200
#   rnn_layers: 1
#   rnn_units: 20
#   dense_layers: 0 # hidden dense layers AFTER recurrent layers and BEFORE final output cell
#   dense_units: 1
#   activation: ['linear', 'linear']
#   centering: [0.0,0.0]
#   dropout: [0.0, 0.0]  # NOTE: length must match total number of layers, default is 1 hidden recurrent layer and 1 dense output layer
#   recurrent_dropout: 0.0 # Length must match number of recurrent layers
#   reset_states: True # reset hidden states after training epoch, triggers reset_states() via callbacks
#   epochs: 200
#   learning_rate: 0.001
#   phys_initialize: False # physics initialization
#   stateful: True
#   verbose_weights: True # Prints out hashs of weights for tracking reproducibility
#   verbose_fit: False # Prints out all training epochs, makes computation much slower
#   features_list: ['Ed', 'Ew', 'rain']
#   scale: True
#   scaler: 'reproducibility'
#   train_frac: 0.5
#   val_frac: 0.0


# Other ML Params
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

xgb:
  max_depth: 3
  eta: 0.1
  min_child_weight: 1
  subsample: 0.8
  colsample_bytree: 0.9
  scale_pos_weight: 1
  n_estimators: 100
  gamma: .1
  scaler: 'standard'
  features_list: ['Ed', 'Ew', 'solar', 'wind', 'rain']

  ### Params sent by Schreck, slow and less accurate for this dataset
    # objective: "reg:squarederror"
    # n_splits: 1
    # learning_rate: 0.1 
    # n_estimators: 1000
    # max_depth: 10
    # n_jobs: 8
    # colsample_bytree: 0.8995496645826047
    # gamma: 0.6148001693726943
    # learning_rate: 0.07773680788294579
    # max_depth: 10 
    # subsample: 0.7898672617361431
    # metric: "valid_rmse"

rf:
  n_estimators: 25 # Number of trees in the forest
  criterion: "squared_error" # Function to measure the quality of a split (previously "mse")
  max_depth: 5 # Maximum depth of the tree
  min_samples_split: 2 # Minimum number of samples required to split an internal node
  min_samples_leaf: 1 # Minimum number of samples required to be at a leaf node
  max_features: .8 # Number of features to consider when looking for the best split
  bootstrap: true # Whether bootstrap samples are used when building trees
  max_samples: null # If bootstrap is True, the number of samples to draw from X to train each base estimator
  random_state: null # Controls both the randomness of the bootstrapping of the samples and the sampling of the features
  verbose: 0 # Controls the verbosity when fitting and predicting
  warm_start: false # When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble
  scaler: null
  features_list: ['Ed', 'Ew', 'solar', 'wind', 'rain']
  
lm:
  fit_intercept: true
  scaler: null
  features_list: ['Ed', 'Ew', 'solar', 'wind', 'rain']
  






  